{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HW2_2021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "edk_oVg0lrtW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpXmBjIHDyk0"
      },
      "source": [
        "## Building a deep learning calculator\n",
        "\n",
        "In this HW we will use seq2seq for building a calculator. The input will be an equation and the solution will be generated by the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yUUpCXrD5Tv"
      },
      "source": [
        "### Data Generation\n",
        "\n",
        "In this task we will generate our own data! We will use three operators (addition, multiplication and subtraction) and work with positive integer numbers in some range. Here are examples of correct inputs and outputs:\n",
        "\n",
        "    Input: '1+2'\n",
        "    Output: '3'\n",
        "    \n",
        "    Input: '0-99'\n",
        "    Output: '-99'\n",
        "\n",
        "*Note, that there are no spaces between operators and operands.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-Ps0hhd3p6k"
      },
      "source": [
        "import random"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1oF_FI43p6o"
      },
      "source": [
        "def generate_equations(allowed_operators, dataset_size, min_value, max_value):\n",
        "    \"\"\"Generates pairs of equations and solutions to them.\n",
        "    \n",
        "       Each equation has a form of two integers with an operator in between.\n",
        "       Each solution is an integer with the result of the operaion.\n",
        "    \n",
        "        allowed_operators: list of strings, allowed operators.\n",
        "        dataset_size: an integer, number of equations to be generated.\n",
        "        min_value: an integer, min value of each operand.\n",
        "        max_value: an integer, max value of each operand.\n",
        "\n",
        "        result: a list of tuples of strings (equation, solution).\n",
        "    \"\"\"\n",
        "    sample = []\n",
        "    for _ in range(dataset_size):\n",
        "        left_operand = str(random.randint(min_value, max_value))\n",
        "        right_operand = str(random.randint(min_value, max_value))\n",
        "        operator = random.choice(allowed_operators)\n",
        "        operation = left_operand+operator+right_operand\n",
        "        sample.append((operation, str(eval(operation))))\n",
        "    return sample"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_NL2hsKEGOk"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "allowed_operators = ['+', '-','*']\n",
        "dataset_size = 100000\n",
        "data = generate_equations(allowed_operators, dataset_size, min_value=0, max_value=10000)\n",
        "\n",
        "train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK4MK0ulEe57",
        "outputId": "0fb4ebe7-5499-428a-d155-bafc65119110"
      },
      "source": [
        "train_set[0:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('7687-1741', '5946'),\n",
              " ('1576+4576', '6152'),\n",
              " ('4145*5128', '21255560'),\n",
              " ('4420+8012', '12432'),\n",
              " ('7131-2368', '4763'),\n",
              " ('5702-1497', '4205'),\n",
              " ('5516+6848', '12364'),\n",
              " ('5149-4854', '295'),\n",
              " ('9690*5204', '50426760'),\n",
              " ('4994*4677', '23356938')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UPW3sV8lrsb"
      },
      "source": [
        "### Building vocabularies and tokenization function\n",
        "\n",
        "We now need to build vocabularies that map strings to token ids and vice versa. We're gonna need these  when we feed training data into  the model or convert output matrices into words. \n",
        "\n",
        "Pay a close attention to the special characters you need to add for the vocabulary:\n",
        "\n",
        "\n",
        "*    End of equation / solution token\n",
        "*    Begining of equation / solution token\n",
        "*    Padding token\n",
        "\n",
        "Please note that in the exercise we do not need the <UNK> token\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmTy_m_olrsb"
      },
      "source": [
        "#build a vocabulary  string --> tokenId\n",
        "#build a reverse vocabulary  tokenId --> string\n",
        "#build a tokenizer (i.e. a function which takes a string and returns tokenids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHWgx34flrsn"
      },
      "source": [
        "### Encoder-decoder model\n",
        "\n",
        "The code below contains a template for a simple encoder-decoder model: single GRU encoder/decoder.\n",
        "**Please note that some places require change and your implementation.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd_rDRm9lrso"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgfN5-F7lrst"
      },
      "source": [
        "class BasicModel(nn.Module):\n",
        "    def __init__(self, vocab, emb_size=64, hid_size=128):\n",
        "        \"\"\"\n",
        "        A simple encoder-decoder seq2seq model\n",
        "        \"\"\"\n",
        "        super().__init__() # initialize base class to track sub-layers, parameters, etc.\n",
        "\n",
        "        self.inp_voc = vocab\n",
        "        self.hid_size = hid_size\n",
        "        \n",
        "        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n",
        "        self.emb_out = nn.Embedding(len(inp_voc), emb_size)\n",
        "        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n",
        "\n",
        "        self.dec_start = nn.Linear(hid_size, hid_size)\n",
        "        self.dec0 = nn.GRUCell(emb_size, hid_size)\n",
        "        self.logits = nn.Linear(hid_size, len(inp_voc))\n",
        "        \n",
        "    def forward(self, inp, out):\n",
        "        \"\"\" Apply model in training mode \"\"\"\n",
        "        initial_state = self.encode(inp)\n",
        "        return self.decode(initial_state, out)\n",
        "\n",
        "\n",
        "    def encode(self, inp, **flags):\n",
        "        \"\"\"\n",
        "        Takes symbolic input sequence, computes initial state\n",
        "        :param inp: matrix of input tokens [batch, time]\n",
        "        :returns: initial decoder state tensors, one or many\n",
        "        \"\"\"\n",
        "        inp_emb = self.emb_inp(inp)\n",
        "        batch_size = inp.shape[0]\n",
        "        \n",
        "        enc_seq, [last_state_but_not_really] = self.enc0(inp_emb)\n",
        "        # enc_seq: [batch, time, hid_size], last_state: [batch, hid_size]\n",
        "        \n",
        "        # note: last_state is not _actually_ last because of padding, let's find the real last_state\n",
        "        lengths = (inp != self.inp_voc.eos_ix).to(torch.int64).sum(dim=1).clamp_max(inp.shape[1] - 1)\n",
        "        last_state = enc_seq[torch.arange(len(enc_seq)), lengths]\n",
        "        # ^-- shape: [batch_size, hid_size]\n",
        "        \n",
        "        dec_start = self.dec_start(last_state)\n",
        "        return [dec_start]\n",
        "\n",
        "    def decode_step(self, prev_state, prev_tokens, **flags):\n",
        "        \"\"\"\n",
        "        Takes previous decoder state and tokens, returns new state and logits for next tokens\n",
        "        :param prev_state: a list of previous decoder state tensors, same as returned by encode(...)\n",
        "        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n",
        "        :return: a list of next decoder state tensors, a tensor of logits [batch, len(inp_voc)]\n",
        "        \"\"\"\n",
        "        prev_gru0_state = prev_state[0]\n",
        "        \n",
        "        <YOUR CODE HERE>\n",
        "        \n",
        "        return new_dec_state, output_logits\n",
        "\n",
        "    def decode(self, initial_state, out_tokens, **flags):\n",
        "        \"\"\" Iterate over reference tokens (out_tokens) with decode_step \"\"\"\n",
        "        batch_size = out_tokens.shape[0]\n",
        "        state = initial_state\n",
        "        \n",
        "        # initial logits: always predict BOS\n",
        "        onehot_bos = F.one_hot(torch.full([batch_size], self.inp_voc.bos_ix, dtype=torch.int64),\n",
        "                               num_classes=len(self.inp_voc)).to(device=out_tokens.device)\n",
        "        first_logits = torch.log(onehot_bos.to(torch.float32) + 1e-9)\n",
        "        \n",
        "        logits_sequence = [first_logits]\n",
        "        for i in range(out_tokens.shape[1] - 1):\n",
        "            state, logits = self.decode_step(state, out_tokens[:, i])\n",
        "            logits_sequence.append(logits)\n",
        "        return torch.stack(logits_sequence, dim=1)\n",
        "\n",
        "    def decode_inference(self, initial_state, max_len=100, **flags):\n",
        "        \"\"\" Generate solutions from model (greedy version) \"\"\"\n",
        "        batch_size, device = len(initial_state[0]), initial_state[0].device\n",
        "        state = initial_state\n",
        "        outputs = [torch.full([batch_size], self.inp_voc.bos_ix, dtype=torch.int64, \n",
        "                              device=device)]\n",
        "        all_states = [initial_state]\n",
        "\n",
        "        for i in range(max_len):\n",
        "            state, logits = self.decode_step(state, outputs[-1])\n",
        "            outputs.append(logits.argmax(dim=-1))\n",
        "            all_states.append(state)\n",
        "        \n",
        "        return torch.stack(outputs, dim=1), all_states\n",
        "\n",
        "    def caculate_lines(self, inp_lines, **kwargs):\n",
        "        inp = self.inp_voc.to_matrix(inp_lines).to(device)\n",
        "        initial_state = self.encode(inp)\n",
        "        out_ids, states = self.decode_inference(initial_state, **kwargs)\n",
        "        return self.inp_voc.to_lines(out_ids.cpu().numpy()), states\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQDhGwg4lrtC"
      },
      "source": [
        "### Training loop\n",
        "\n",
        "Training encoder-decoder models isn't that different from any other models: sample batches, compute loss, backprop and update.\n",
        "\n",
        "For training loss we will use ***torch.nn.NLLLoss*** please note that the loss should not be calculated with the padding token. (For ignoring specific labels please look at ***torch.nn.NLLLoss*** documentation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsoV49PcKBmA"
      },
      "source": [
        "#<Implement training loop>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz9aROAIlrtX"
      },
      "source": [
        "## Adding Attention Layer\n",
        "Here you will have to implement a layer that computes a simple additive attention:\n",
        "\n",
        "Given encoder sequence $ h^e_0, h^e_1, h^e_2, ..., h^e_T$ and a single decoder state $h^d$,\n",
        "\n",
        "* Compute logits with a 2-layer neural network\n",
        "$$a_t = linear_{out}(tanh(linear_{e}(h^e_t) + linear_{d}(h_d)))$$\n",
        "* Get probabilities from logits, \n",
        "$$ p_t = {{e ^ {a_t}} \\over { \\sum_\\tau e^{a_\\tau} }} $$\n",
        "\n",
        "* Add up encoder states with probabilities to get __attention response__\n",
        "$$ attn = \\sum_t p_t \\cdot h^e_t $$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6j-4ZhM2KQpy"
      },
      "source": [
        "#<implement attention layer>\n",
        "#<add attention layer for your seq2seq model>\n",
        "#<Train the two models (with/without attention and compare the results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v63Jds6lY8iD"
      },
      "source": [
        "## Good Luck!"
      ]
    }
  ]
}