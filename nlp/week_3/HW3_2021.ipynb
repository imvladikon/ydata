{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUfalD_UxOm3"
      },
      "source": [
        "# HW3\n",
        "\n",
        "The goals of this homework are:\n",
        "1. Get acquainted with the transformers package and with the implemntation of BERT. \n",
        "2. Implement Adapters for transfer learning.\n",
        "4. Get familiar with the GLUE benchmark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd4p2DiRxulW"
      },
      "source": [
        "## Problem with fine tuning the entire model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyYP_nmKxl0y"
      },
      "source": [
        "As we learned in the last lecture fine-tuning large pretrained models is the most effective transfer learning approch. However, in case we have a lot of downstream tasks the above method can be cumbersome since for each new task an entire new model needs to be trained.\n",
        "\n",
        "## Adapters - Paramter Efficent approach\n",
        " In this exercise we will implement another method for transfer learning which only adds few trainable parameters per task, and new tasks can be added without revisiting previous ones. \n",
        " \n",
        "The method was proposed at the following paper https://arxiv.org/pdf/1902.00751.pdf and the details regarding the implmentation can be found in section 2.1 (you are also advised to look at section 3 for tuning the parameters)\n",
        "\n",
        "**Please note that in addition to the adapter layers the autorhs also  trained a new layer normalization - For our excercise implementing this part is not mandatory!**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euJzu9aMjuPN"
      },
      "source": [
        "In this excercise you are required to:\n",
        "1. Implement adapter layer\n",
        "2. Modify the Implementation of BERT to include the adapter layer (clone the transformers package and use the exisiting BERT implemntation which can be found in the following path ./src/transformers/models/bert/modeling_bert.py ) \n",
        "3. Finetune BERT on the GLUE benchmark with and without Adapters and report the results (see https://colab.research.google.com/github/huggingface/notebooks/blob/master/examples/text_classification.ipynb  for reference on using the GLUE benchmark)  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8o-ffHUKpet"
      },
      "source": [
        "#Good Luck!"
      ]
    }
  ]
}